# Kancil AI - Environment Configuration
# Copy this file to .env and fill in your values

# ============================================================
# LLM Provider Configuration
# ============================================================
# Choose your LLM provider: groq, openrouter, openai, or ollama
LLM_PROVIDER=groq

# API Key (not needed for ollama)
LLM_API_KEY=your-api-key-here

# Base URL (optional, defaults based on provider)
# LLM_BASE_URL=https://api.groq.com/openai/v1

# Model name (optional, defaults based on provider)
# LLM_MODEL=llama-3.1-8b-instant

# ============================================================
# Provider-Specific Settings
# ============================================================
# For Groq:
# LLM_PROVIDER=groq
# LLM_API_KEY=gsk_...
# LLM_MODEL=llama-3.1-8b-instant

# For OpenRouter:
# LLM_PROVIDER=openrouter
# LLM_API_KEY=sk-or-...
# LLM_MODEL=meta-llama/llama-3.1-8b-instruct

# For OpenAI:
# LLM_PROVIDER=openai
# LLM_API_KEY=sk-...
# LLM_MODEL=gpt-3.5-turbo

# For Ollama (local):
# LLM_PROVIDER=ollama
# LLM_BASE_URL=http://127.0.0.1:11434
# LLM_MODEL=llama2

# ============================================================
# Optional Settings
# ============================================================
FLASK_SECRET_KEY=your-secret-key-here
DEBUG=false
PORT=5050

# Site metadata for OpenRouter
SITE_URL=https://example.com
SITE_NAME=Kancil AI
